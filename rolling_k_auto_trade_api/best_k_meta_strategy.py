# -*- coding: utf-8 -*-
# best_k_meta_strategy.py (ì‹¤ì „ rolling_k, ìµœì í™” ì „ì²´ë³¸)
"""
ì‹¤ì „í˜• rolling_k ë³€ë™ì„±ëŒíŒŒ + ì›”ì´ˆ/rolling/TopN/ë³´ìœ ë¶„/ë™ì K/ê°€ì¤‘ì¹˜ ìµœì í™” ì „ëµ
- KOSDAQ TopN(pykrx+fdr) ìœ ë‹ˆë²„ìŠ¤/ì‹œì´ ë™ì 
- ì›”/ë¶„ê¸°/ì—°ê°„ K-grid(ê³ ì •/ATRë™ì )
- ëª©í‘œê°€: ì „ì¼ ë³€ë™í­*K + í‹±ë³´ì •
- best_k/Sharpe/ìŠ¹ë¥ /ìˆ˜ìµë¥ /MDD/ê±°ë˜ìˆ˜ í•„í„° + assign_weights
- ë³´ìœ ì¢…ëª© ê°•ì œí¬í•¨/ë¹„ì¤‘í•˜í•œ/rolling í†µí•©
- FastAPI(trader.py/main.py)ì—ì„œ /rebalance/run/{date}ê°€ í˜¸ì¶œí•  run_rebalance() ì œê³µ
"""

from __future__ import annotations

import logging
import math
import os
from datetime import datetime, timedelta, date
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional
import json

import numpy as np
import pandas as pd
import FinanceDataReader as fdr
import requests
from pykrx.stock import (
    get_market_cap_by_ticker,
    get_nearest_business_day_in_a_week,
)
try:  # pykrx wrapperì˜ ì˜ëª»ëœ logging í¬ë§·ìœ¼ë¡œ ì¸í•œ ë¡œê·¸ í­ì£¼ ë°©ì§€
    from pykrx.website import comm as _pykrx_comm  # type: ignore

    if hasattr(_pykrx_comm, "logging") and hasattr(_pykrx_comm.logging, "info"):
        _pykrx_comm.logging.info = lambda *_, **__: None  # type: ignore
except Exception:
    # pykrxê°€ ì—†ê±°ë‚˜ ë‚´ë¶€ êµ¬ì¡° ë³€ê²½ ì‹œì—ë„ ëŸ°ì´ ê³„ì†ë˜ë„ë¡ ë¬´ì‹œ
    pass

from trader.rkmax_utils import get_best_k_meta, assign_weights, _enforce_min_weight_for_forced
from .simulate_with_k_and_get_metrics import simulate_with_k_and_get_metrics
from rolling_k_auto_trade_api.adjust_price_to_tick import adjust_price_to_tick

logger = logging.getLogger(__name__)

# -----------------------------
# í™˜ê²½ íŒŒë¼ë¯¸í„° (íŠœë‹ ê°€ëŠ¥)
# -----------------------------
K_MIN = float(os.getenv("K_MIN", "0.1"))
K_MAX = float(os.getenv("K_MAX", "1.0"))
K_STEP = float(os.getenv("K_STEP", "0.1"))
K_GRID_MODE = os.getenv("K_GRID_MODE", "fixed").lower()  # fixed|fine|atr
K_STEP_FINE = float(os.getenv("K_STEP_FINE", "0.05"))
K_DYNAMIC_STEP_MIN = float(os.getenv("K_DYNAMIC_STEP_MIN", "0.03"))
K_DYNAMIC_STEP_MAX = float(os.getenv("K_DYNAMIC_STEP_MAX", "0.10"))
K_DYNAMIC_STEP_MULT = float(os.getenv("K_DYNAMIC_STEP_MULT", "1.5"))

MIN_TRADES = int(os.getenv("MIN_TRADES", "5"))
MAX_MDD_PCT = float(os.getenv("MAX_MDD_PCT", "30"))
REQUIRE_POS_RET = os.getenv("REQUIRE_POS_RET", "true").lower() == "true"

TOP_N = int(os.getenv("TOP_N", "50"))

ALWAYS_INCLUDE_CODES = {
    c.strip() for c in os.getenv("ALWAYS_INCLUDE_CODES", "").replace(" ", "").split(",") if c.strip()
}
KEEP_HELD_BYPASS_FILTERS = os.getenv("KEEP_HELD_BYPASS_FILTERS", "true").lower() == "true"
HELD_MIN_WEIGHT = float(os.getenv("HELD_MIN_WEIGHT", "0.01"))
UNIVERSE_CACHE_ENV = "UNIVERSE_CACHE_DIR"
UNIVERSE_CACHE_SUBDIR = "universe_cache"

# -----------------------------
# ìœ í‹¸
# -----------------------------
def _clip(v: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, v))

def _round2(x: float) -> float:
    return float(np.round(x, 2))

def _safe_float(x: Any, default: float | None = 0.0) -> float | None:
    try:
        return float(x)
    except Exception:
        return default

def _find_column(df: pd.DataFrame, keyword: str) -> Optional[str]:
    kw = keyword.replace(" ", "")
    for c in df.columns:
        if kw in str(c).replace(" ", ""):
            return c
    return None

# -----------------------------
# 1) ì‹œê°€ì´ì•¡ ê¸°ì¤€ Top-N (KOSDAQ only for rolling-k universe)
# -----------------------------
@lru_cache(maxsize=None)
def _get_listing_df_cached(markets: tuple[str, ...]) -> pd.DataFrame:
    frames: List[pd.DataFrame] = []
    for m in markets:
        try:
            df = fdr.StockListing(m).rename(columns={"Symbol": "Code", "Name": "Name"})
            df["Code"] = df["Code"].astype(str).str.zfill(6)
            frames.append(df[["Code", "Name"]])
        except Exception:
            logger.exception("âŒ  StockListing(%s) ì‹¤íŒ¨", m)
    if not frames:
        return pd.DataFrame(columns=["Code", "Name"])
    merged = pd.concat(frames, ignore_index=True)
    merged = merged.drop_duplicates(subset=["Code"], keep="first")
    return merged


def _get_listing_df(markets: Iterable[str]) -> pd.DataFrame:
    """ì£¼ì–´ì§„ ì‹œì¥ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ì¢…ëª©ëª… DF í•©ì¹œ í›„ Code í¬ë§·ì„ ì •ê·œí™”í•œë‹¤."""
    normalized_markets = tuple(dict.fromkeys(markets))
    return _get_listing_df_cached(normalized_markets).copy()


def _universe_cache_base() -> Path:
    explicit = os.getenv(UNIVERSE_CACHE_ENV)
    if explicit:
        return Path(explicit)
    base_dir = Path(os.getenv("LEDGER_BASE_DIR", "bot_state/trader_ledger"))
    if not base_dir.is_absolute():
        base_dir = Path.cwd() / base_dir
    return base_dir / UNIVERSE_CACHE_SUBDIR


def _universe_cache_path(market: str) -> Path:
    return _universe_cache_base() / market / "latest.json"


def _load_cached_universe(market: str) -> pd.DataFrame:
    path = _universe_cache_path(market)
    try:
        if path.exists():
            payload = json.loads(path.read_text())
            df = pd.DataFrame(payload)
            if not df.empty:
                df["Code"] = df["Code"].astype(str).str.zfill(6)
            return df
    except Exception:
        logger.warning("âš ï¸  %s ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ â†’ ë¹ˆ DF ì‚¬ìš©", market, exc_info=logger.isEnabledFor(logging.DEBUG))
    return pd.DataFrame(columns=["Code", "Name", "Marcap"])


def _save_cached_universe(df: pd.DataFrame, market: str) -> None:
    if df is None or df.empty:
        return
    path = _universe_cache_path(market)
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_suffix(".tmp")
    try:
        tmp_path.write_text(df.to_json(orient="records", force_ascii=False))
        tmp_path.replace(path)
        logger.info("ğŸ’¾ %s ìœ ë‹ˆë²„ìŠ¤ ìºì‹œ ì €ì¥ %s", market, path)
    except Exception:
        logger.warning("âš ï¸  %s ìºì‹œ ì €ì¥ ì‹¤íŒ¨", market, exc_info=logger.isEnabledFor(logging.DEBUG))
        try:
            if tmp_path.exists():
                tmp_path.unlink()
        except Exception:
            pass

def _get_top_n_for_market(date_str: Optional[str], n: int, market: str) -> pd.DataFrame:
    """ì£¼ì–´ì§„ ì‹œì¥ì˜ ì‹œê°€ì´ì•¡ ìƒìœ„ nê°œ ì¢…ëª© ë°˜í™˜."""
    cached = _load_cached_universe(market)
    try:
        target_dt = datetime.today() if date_str is None else datetime.strptime(date_str, "%Y-%m-%d")
        from_date = get_nearest_business_day_in_a_week(target_dt.strftime("%Y%m%d"))
        logger.info(f"ğŸ“… pykrx ì‹œì´ ì¡°íšŒì¼({market}) â†’ {from_date}")

        mktcap_df = get_market_cap_by_ticker(from_date, market=market)
        if mktcap_df is None or len(mktcap_df) == 0:
            logger.warning("âš ï¸  pykrx ì‹œì´ DF(%s)ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤ â†’ ë¹ˆ DF ë°˜í™˜", market)
            return cached if not cached.empty else pd.DataFrame(columns=["Code", "Name", "Marcap"])

        mktcap_df = mktcap_df.reset_index()
        capcol = _find_column(mktcap_df, "ì‹œê°€ì´ì•¡")
        ticcol = _find_column(mktcap_df, "í‹°ì»¤") or _find_column(mktcap_df, "ì½”ë“œ")
        if capcol is None or ticcol is None:
            logger.error("âŒ  %s ì‹œì´/í‹°ì»¤ ì»¬ëŸ¼ íƒìƒ‰ ì‹¤íŒ¨ â†’ ë¹ˆ DF ë°˜í™˜", market)
            return cached if not cached.empty else pd.DataFrame(columns=["Code", "Name", "Marcap"])

        mktcap_df = mktcap_df.rename(columns={capcol: "Marcap", ticcol: "Code"})
        mktcap_df["Code"] = mktcap_df["Code"].astype(str).str.zfill(6)

        fdr_df = _get_listing_df([market])
        merged = pd.merge(
            fdr_df[["Code", "Name"]],
            mktcap_df[["Code", "Marcap"]],
            on="Code",
            how="inner",
        )
        if "Marcap" not in merged.columns:
            for cand in ("Marcap_x", "Marcap_y", "MarketCap", "MarketCap_x", "MarketCap_y"):
                if cand in merged.columns:
                    merged = merged.rename(columns={cand: "Marcap"})
                    break
        if "Marcap" not in merged.columns:
            logger.error("âŒ  ë³‘í•© í›„ì—ë„ 'Marcap' ì—†ìŒ(%s) â†’ ë¹ˆ DF ë°˜í™˜", market)
            return cached if not cached.empty else pd.DataFrame(columns=["Code", "Name", "Marcap"])

        topn = merged.dropna(subset=["Marcap"])
        # 6ìë¦¬ ìˆ«ì ì½”ë“œë§Œ ì‚¬ìš© (ìš°ì„ ì£¼/ETN ë“± íŠ¹ìˆ˜ì½”ë“œ, 0009K0 ê°™ì€ ê²ƒ ì œê±°)
        topn = topn[topn["Code"].astype(str).str.match(r"^\d{6}$")]
        topn = topn.sort_values("Marcap", ascending=False).head(n)
        logger.info(f"âœ…  {market} ì‹œì´ Top{n} ì¶”ì¶œ ì™„ë£Œ â†’ {len(topn)} ì¢…ëª©")
        result = topn[["Code", "Name", "Marcap"]]
        if result.empty and not cached.empty:
            logger.warning("âš ï¸  %s TopN ê²°ê³¼ê°€ ë¹„ì–´ ìºì‹œ ì‚¬ìš©(%d rows)", market, len(cached))
            return cached
        _save_cached_universe(result, market)
        return result

    except (
        requests.exceptions.JSONDecodeError,
        json.decoder.JSONDecodeError,
        IndexError,
        ValueError,
    ) as exc:
        logger.warning("âš ï¸  %s pykrx ì¡°íšŒ ì‹¤íŒ¨ â†’ ìºì‹œ í´ë°± ì‹œë„: %s", market, exc)
    except Exception:
        logger.warning("âš ï¸  get_top_n_for_market(%s) ì˜ˆì™¸ ë°œìƒ â†’ ìºì‹œ í´ë°±", market, exc_info=logger.isEnabledFor(logging.DEBUG))

    if not cached.empty:
        logger.info("â†©ï¸  %s ìœ ë‹ˆë²„ìŠ¤ ìºì‹œ ì‚¬ìš© (%d rows)", market, len(cached))
        return cached
    logger.warning("âš ï¸  %s ìºì‹œ ì—†ìŒ â†’ ë¹ˆ DF ë°˜í™˜", market)
    return pd.DataFrame(columns=["Code", "Name", "Marcap"])

def get_kosdaq_top_n(date_str: Optional[str] = None, n: int = TOP_N) -> pd.DataFrame:
    return _get_top_n_for_market(date_str, n, market="KOSDAQ")

def get_kospi_top_n(date_str: Optional[str] = None, n: int = TOP_N) -> pd.DataFrame:
    return _get_top_n_for_market(date_str, n, market="KOSPI")

# -----------------------------
# ATR ê³„ì‚°(ì›” ë°ì´í„° ë ˆì½”ë“œì—ì„œ)
# -----------------------------
def _compute_atr_from_records(records: List[Dict[str, Any]], window: int = 14) -> Optional[float]:
    """ì›” êµ¬ê°„ ë ˆì½”ë“œ([{open,high,low,close}...])ì—ì„œ ATR ê³„ì‚°."""
    if not records or len(records) < window + 1:
        return None
    df = pd.DataFrame(records).copy()
    need = {"open", "high", "low", "close"}
    if not need.issubset(set(df.columns)):
        return None
    df = df[["open", "high", "low", "close"]].astype(float)
    prev_close = df["close"].shift(1)
    tr = pd.concat(
        [
            (df["high"] - df["low"]).abs(),
            (df["high"] - prev_close).abs(),
            (df["low"] - prev_close).abs(),
        ],
        axis=1,
    ).max(axis=1)
    atr = tr.rolling(window=window, min_periods=window).mean().iloc[-1]
    try:
        return float(atr) if atr and not math.isnan(atr) else None
    except Exception:
        return None

# -----------------------------
# K ê·¸ë¦¬ë“œ ìƒì„±
# -----------------------------
def _build_k_range(code: str, month_data: List[Dict[str, Any]]) -> np.ndarray:
    kmin, kmax = float(K_MIN), float(K_MAX)
    kmin = _clip(kmin, 0.01, 1.50)
    kmax = _clip(kmax, 0.05, 1.50)
    if kmax <= kmin:
        kmax = kmin + 0.05

    mode = K_GRID_MODE
    step = float(K_STEP)
    if mode == "fine":
        step = float(K_STEP_FINE)
    elif mode == "atr":
        atr = _compute_atr_from_records(month_data, window=14)
        close = _safe_float(month_data[-1].get("close")) if month_data else None
        if atr and close and close > 0:
            step_est = K_DYNAMIC_STEP_MULT * (atr / close)
            step = _clip(_round2(step_est), K_DYNAMIC_STEP_MIN, K_DYNAMIC_STEP_MAX)
        else:
            step = float(K_STEP_FINE)

    steps = int(round((kmax - kmin) / max(1e-6, step))) + 1
    steps = int(_clip(steps, 3, 100))
    k_range = np.round(np.linspace(kmin, kmax, steps), 2)
    k_range = np.unique(np.clip(k_range, 0.01, 1.50))
    logger.debug(f"[KGRID] {code} mode={mode} range=[{kmin:.2f},{kmax:.2f}] stepâ‰ˆ{step:.2f} â†’ {len(k_range)} pts")
    return k_range

# -----------------------------
# 2) K ì‹œë®¬ë ˆì´ì…˜ (ì›” êµ¬ê°„)
# -----------------------------
def simulate_k_range_for(
    code: str,
    price_data: List[Dict[str, Any]],
    k_range: Optional[np.ndarray] = None,
) -> List[Dict[str, Any]]:
    results: List[Dict[str, Any]] = []
    if not price_data:
        return results
    if k_range is None:
        k_range = _build_k_range(code, price_data)
    for k in k_range:
        metrics = simulate_with_k_and_get_metrics(code, float(k), price_data)
        metrics["k"] = float(k)
        try:
            mu = float(metrics.get("avg_return_pct", 0)) / 100.0
            mdd = abs(float(metrics.get("mdd_pct", 0))) / 100.0
            sharpe = (mu) / (0.01 + mdd)
            metrics["sharpe"] = round(sharpe, 4)
        except Exception:
            metrics["sharpe"] = 0.0
        results.append(metrics)
    return results

# -----------------------------
# 3) ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ (1ë…„Â·1ë¶„ê¸°Â·1ê°œì›”)
# -----------------------------
def get_price_data_segments(code: str, base_date: date) -> Dict[str, List[Dict[str, Any]]]:
    """
    base_dateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ê±°ë˜ì¼ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬
    year/quarter/month ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë°˜í™˜.
    """
    try:
        start_date = base_date - timedelta(days=400)
        end_date = base_date - timedelta(days=1)
        df = fdr.DataReader(code, start=start_date, end=end_date)
        df = (
            df.dropna(subset=["Open", "High", "Low", "Close", "Volume"])
            .rename(columns={
                "Open": "open", "High": "high", "Low": "low",
                "Close": "close", "Volume": "volume"
            })
            .reset_index()
        )
        if "Date" not in df.columns:
            if df.index.name is not None:
                df = df.rename_axis("Date").reset_index()
            else:
                raise ValueError("DataReader ê²°ê³¼ì— Date ì»¬ëŸ¼ì´ ì—†ìŒ")
        df["date"] = pd.to_datetime(df["Date"]).dt.date
        df = df[["date", "open", "high", "low", "close", "volume"]].sort_values("date")
        prev_records = df[df["date"] < base_date].tail(1).to_dict("records")
        return {
            "year": df[df["date"] >= base_date - timedelta(days=365)].to_dict("records"),
            "quarter": df[df["date"] >= base_date - timedelta(days=90)].to_dict("records"),
            "month": df[df["date"] >= base_date - timedelta(days=30)].to_dict("records"),
            "prev": prev_records,
        }
    except Exception as e:
        logger.exception(f"[ERROR] âŒ Failed to fetch data for {code}: {e}")
        return {"year": [], "quarter": [], "month": [], "prev": []}

# -----------------------------
# 4) K ìµœì í™” & í•„í„°ë§ (+ ë³´ìœ ë¶„ ê°•ì œ í¬í•¨)
# -----------------------------
def _parse_force_include_codes(env_codes: Iterable[str]) -> List[str]:
    out = []
    for c in env_codes:
        c = str(c).strip()
        if not c:
            continue
        out.append(c.zfill(6))
    return sorted(set(out))

def _inject_forced_codes(universe_df: pd.DataFrame, forced_codes: List[str], markets: Iterable[str]) -> pd.DataFrame:
    if not forced_codes:
        return universe_df
    fdr_df = _get_listing_df(list(markets))
    force_df = fdr_df[fdr_df["Code"].isin(forced_codes)][["Code", "Name"]].copy()
    missing = [c for c in forced_codes if c not in set(force_df["Code"])]
    if missing:
        force_df = pd.concat(
            [force_df, pd.DataFrame({"Code": missing, "Name": [None] * len(missing)})],
            ignore_index=True,
        )
    uni = universe_df.copy()
    uni = pd.concat([uni[["Code", "Name", "Marcap"]], force_df.assign(Marcap=np.nan)], ignore_index=True)
    uni = uni.drop_duplicates(subset=["Code"], keep="first")
    return uni

def _calc_best_k_for_universe(
    universe_df: pd.DataFrame,
    rebalance_date: date,
    forced_codes: List[str],
    market: str,
) -> List[Dict[str, Any]]:
    results: Dict[str, Dict[str, Any]] = {}

    for _, stock in universe_df.iterrows():
        code, name = str(stock["Code"]).zfill(6), stock.get("Name")
        try:
            segments = get_price_data_segments(code, rebalance_date)
            month_data = segments["month"]

            if not month_data:
                logger.debug(f"[SKIP] {name}({code}) ì „ì›” ë°ì´í„° ì—†ìŒ")
                if code in forced_codes and KEEP_HELD_BYPASS_FILTERS:
                    results[code] = {
                        "code": code, "name": name, "market": market, "best_k": 0.5,
                        "avg_return_pct": 0.0, "win_rate_pct": 0.0,
                        "mdd_pct": 0.0, "trades": 0, "cumulative_return_pct": 0.0,
                        "avg_holding_days": 0.0, "sharpe_m": 0.0,
                        "ëª©í‘œê°€": None, "close": None,
                        "prev_open": None, "prev_high": None, "prev_low": None, "prev_close": None, "prev_volume": None, "prev_turnover": None,
                        "forced_include": True, "filtered_reason": "NO_DATA",
                        "qty": None, "weight": None, "k_grid_mode": K_GRID_MODE,
                    }
                continue

            # K grid â†’ best_k ì„ íƒ
            k_range = _build_k_range(code, month_data)
            m_metrics = simulate_k_range_for(code, month_data, k_range=k_range)
            best_k = get_best_k_meta([], [], m_metrics)

            # ì„±ëŠ¥ ì§€í‘œ(ì›”)
            month_perf = simulate_with_k_and_get_metrics(code, best_k, month_data)
            avg_return = float(month_perf.get("avg_return_pct", 0.0))
            win_rate = float(month_perf.get("win_rate_pct", 0.0))
            mdd = float(abs(month_perf.get("mdd_pct", 0.0)))
            trades = int(month_perf.get("trades", 0))
            cumret = float(month_perf.get("cumulative_return_pct", 0.0))
            sharpe_m = float(month_perf.get("sharpe_m", 0.0))
            avg_hold = float(month_perf.get("avg_holding_days", 0.0))

            # ë°ì´í„° ë¶€ì¡± or í•„í„°ë§
            if trades < MIN_TRADES:
                logger.debug(f"[SKIP] {name}({code}) trades<{MIN_TRADES}")
                if code in forced_codes and KEEP_HELD_BYPASS_FILTERS:
                    results[code] = {
                        "code": code, "name": name, "market": market, "best_k": best_k,
                        "avg_return_pct": avg_return, "win_rate_pct": win_rate,
                        "mdd_pct": mdd, "trades": trades, "cumulative_return_pct": cumret,
                        "avg_holding_days": avg_hold, "sharpe_m": sharpe_m,
                        "ëª©í‘œê°€": None, "close": None,
                        "prev_open": None, "prev_high": None, "prev_low": None, "prev_close": None, "prev_volume": None, "prev_turnover": None,
                        "forced_include": True, "filtered_reason": "LOW_TRADES",
                        "qty": None, "weight": None, "k_grid_mode": K_GRID_MODE,
                    }
                continue

            if mdd > MAX_MDD_PCT:
                logger.debug(f"[SKIP] {name}({code}) mdd>{MAX_MDD_PCT}")
                if code in forced_codes and KEEP_HELD_BYPASS_FILTERS:
                    results[code] = {
                        "code": code, "name": name, "market": market, "best_k": best_k,
                        "avg_return_pct": avg_return, "win_rate_pct": win_rate,
                        "mdd_pct": mdd, "trades": trades, "cumulative_return_pct": cumret,
                        "avg_holding_days": avg_hold, "sharpe_m": sharpe_m,
                        "ëª©í‘œê°€": None, "close": None,
                        "prev_open": None, "prev_high": None, "prev_low": None, "prev_close": None, "prev_volume": None, "prev_turnover": None,
                        "forced_include": True, "filtered_reason": "HIGH_MDD",
                        "qty": None, "weight": None, "k_grid_mode": K_GRID_MODE,
                    }
                continue

            if REQUIRE_POS_RET and avg_return <= 0:
                logger.debug(f"[SKIP] {name}({code}) avg_return<=0")
                if code in forced_codes and KEEP_HELD_BYPASS_FILTERS:
                    results[code] = {
                        "code": code, "name": name, "market": market, "best_k": best_k,
                        "avg_return_pct": avg_return, "win_rate_pct": win_rate,
                        "mdd_pct": mdd, "trades": trades, "cumulative_return_pct": cumret,
                        "avg_holding_days": avg_hold, "sharpe_m": sharpe_m,
                        "ëª©í‘œê°€": None, "close": None,
                        "prev_open": None, "prev_high": None, "prev_low": None, "prev_close": None, "prev_volume": None, "prev_turnover": None,
                        "forced_include": True, "filtered_reason": "NEG_RETURN",
                        "qty": None, "weight": None, "k_grid_mode": K_GRID_MODE,
                    }
                continue

            # ì „ì¼ OHLCV ë¡œë“œ: 1) month_data ë§ˆì§€ë§‰ ìº”ë“¤, 2) segs["prev"]
            prev_candle = None
            if month_data:
                prev_candle = month_data[-1]
            elif segments.get("prev"):
                prev_candle = segments["prev"][-1]

            prev_open = _safe_float(prev_candle.get("open") if prev_candle else None, None)
            prev_high = _safe_float(prev_candle.get("high") if prev_candle else None, None)
            prev_low = _safe_float(prev_candle.get("low") if prev_candle else None, None)
            prev_close = _safe_float(prev_candle.get("close") if prev_candle else None, None)
            prev_volume = _safe_float(prev_candle.get("volume") if prev_candle else None, None)
            prev_turnover = None
            try:
                if prev_close is not None and prev_volume is not None:
                    prev_turnover = float(prev_close) * float(prev_volume)
            except Exception:
                prev_turnover = None

            # ìµœì¢… ì¶œë ¥
            target_price = adjust_price_to_tick(
                prev_close + (prev_high - prev_low) * best_k,
                code,
            ) if prev_close is not None and prev_high is not None and prev_low is not None else None

            close_price = float(prev_close) if prev_close is not None else None

            results[code] = {
                "code": code,
                "name": name,
                "market": market,
                "best_k": best_k,
                "avg_return_pct": avg_return,
                "win_rate_pct": win_rate,
                "mdd_pct": mdd,
                "trades": trades,
                "cumulative_return_pct": cumret,
                "avg_holding_days": avg_hold,
                "sharpe_m": sharpe_m,
                # trader.pyê°€ ì½ëŠ” í•„ë“œë“¤
                "ëª©í‘œê°€": target_price,                # (ë™ì¼ í‚¤ ìœ ì§€)
                "target_price": target_price,         # í˜¸í™˜ í‚¤ ì¶”ê°€
                "close": close_price,
                "prev_open": prev_open,
                "prev_high": prev_high,
                "prev_low": prev_low,
                "prev_close": prev_close,
                "prev_volume": prev_volume,
                "prev_turnover": prev_turnover,
                # ë©”íƒ€
                "forced_include": code in forced_codes,
                "k_grid_mode": K_GRID_MODE,
                # ìˆ˜ëŸ‰ì€ trader.pyê°€ weightâ†’qtyë¡œ ë³€í™˜í•˜ë¯€ë¡œ ê¸°ë³¸ None
                "qty": None,
                "weight": None,  # assign_weights í›„ ì±„ì›Œì§
            }

            logger.info(
                f"[SIM] {name}({code})[{market}] R={avg_return:.1f}% W={win_rate:.1f}% MDD={mdd:.1f}% "
                f"K={best_k} trades={trades} forced={code in forced_codes}"
            )

        except Exception as e:
            logger.exception(f"[ERR] {name}({code})[{market}] ì‹œë®¬ ì‹¤íŒ¨: {e}")
            continue

    logger.info(f"ğŸ“Š [{market}] í•„í„°/ê°•ì œí¬í•¨ ë°˜ì˜ ì¢…ëª© = {len(results)}ê°œ")
    return list(results.values())


def _normalize_weights(selected: List[Dict[str, Any]], forced_codes: List[str]) -> List[Dict[str, Any]]:
    if not selected:
        return []

    selected = assign_weights(selected)  # ë‚´ë¶€ì—ì„œ 'weight' ì±„ì›Œì§

    # ë³´ìœ ë¶„ ìµœì†Œ ë¹„ì¤‘ í•˜í•œ ë³´ì • (í•©ê³„ 1 ìœ ì§€)
    if HELD_MIN_WEIGHT > 0:
        selected = _enforce_min_weight_for_forced(selected, forced_codes, min_weight=HELD_MIN_WEIGHT)

    # ì‚¬í›„ ì •ê·œí™”ë¡œ weight í•©ê³„ë¥¼ 1.0ìœ¼ë¡œ ìœ ì§€
    total_weight = sum(float(it.get("weight") or 0) for it in selected)
    if total_weight > 0:
        for it in selected:
            it["weight"] = float(it.get("weight") or 0) / total_weight
    return selected


def _normalize_weights_by_market(
    selected_all: List[Dict[str, Any]], forced_codes: List[str]
) -> Dict[str, List[Dict[str, Any]]]:
    """Normalize weights per market without cross-market renormalization."""

    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for row in selected_all:
        market = (row.get("market") or "UNKNOWN").upper()
        grouped.setdefault(market, []).append(row)

    for market, rows in grouped.items():
        grouped[market] = _normalize_weights(rows, forced_codes)
        weight_sum = sum(float(r.get("weight") or 0.0) for r in grouped[market])
        logger.info("[WEIGHT] %s weight_sum=%.6f count=%d", market, weight_sum, len(rows))

    return grouped


def get_best_k_for_kosdaq_topn(rebalance_date_str: str) -> List[Dict[str, Any]]:
    """
    ë¦¬ë°¸ëŸ°ì‹± ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ ì‘ì„±:
    - code/name/best_k/weight(+qty=None) + prev_* + ëª©í‘œê°€(close í¬í•¨)ê¹Œì§€ ì±„ì›€
    - KOSDAQ TopNë§Œ í¬í•¨ (KOSPIëŠ” ë³„ë„ ì½”ì–´ ì—”ì§„ì—ì„œ ì²˜ë¦¬)
    """
    rebalance_date = datetime.strptime(rebalance_date_str, "%Y-%m-%d").date()

    kosdaq_df = get_kosdaq_top_n(rebalance_date_str, n=TOP_N)
    logger.info("ğŸ“ˆ ìœ ë‹ˆë²„ìŠ¤ ìˆ˜ì§‘: KOSDAQ=%d (Top%d)", len(kosdaq_df), TOP_N)
    top_df = kosdaq_df.copy()
    forced_codes = _parse_force_include_codes(ALWAYS_INCLUDE_CODES)
    if forced_codes:
        top_df = _inject_forced_codes(top_df, forced_codes, ["KOSDAQ"])

    if top_df.empty:
        logger.warning("[WARN] KOSDAQ TopN ê²°ê³¼ ì—†ìŒ â†’ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
        return []

    logger.info("ğŸ“Š KOSDAQ ì‹œì´ TopN ìœ ë‹ˆë²„ìŠ¤ ìˆ˜ëŸ‰: %dê°œ (ê³ ìœ )", len(top_df))

    selected = _calc_best_k_for_universe(top_df, rebalance_date, forced_codes, market="KOSDAQ")

    return _normalize_weights(selected, forced_codes)


def get_best_k_for_krx_topn(
    rebalance_date_str: str,
    markets: list[str] | tuple[str, ...] = ("KOSDAQ", "KOSPI"),
    topn_map: dict[str, int] | None = None,
    return_by_market: bool = False,
) -> List[Dict[str, Any]] | Dict[str, Any]:
    """ì‹œì¥ë³„ Top-Nì„ í•©ì³ K ìµœì í™” ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤."""
    rebalance_date = datetime.strptime(rebalance_date_str, "%Y-%m-%d").date()
    markets_seq = list(dict.fromkeys(markets))
    if not markets_seq:
        markets_seq = ["KOSDAQ", "KOSPI"]
    topn_map = topn_map or {"KOSDAQ": TOP_N, "KOSPI": TOP_N}
    forced_codes = _parse_force_include_codes(ALWAYS_INCLUDE_CODES)

    all_selected: List[Dict[str, Any]] = []
    for market in markets_seq:
        n = int(topn_map.get(market, TOP_N))
        uni_df = _get_top_n_for_market(rebalance_date_str, n=n, market=market)
        logger.info("ğŸ“ˆ ìœ ë‹ˆë²„ìŠ¤ ìˆ˜ì§‘: %s=%d (Top%d)", market, len(uni_df), n)
        if forced_codes:
            uni_df = _inject_forced_codes(uni_df, forced_codes, [market])
        if uni_df.empty:
            logger.warning("[WARN] %s TopN ê²°ê³¼ ì—†ìŒ â†’ ê±´ë„ˆëœ€", market)
            continue
        logger.info("ğŸ“Š %s ì‹œì´ TopN ìœ ë‹ˆë²„ìŠ¤ ìˆ˜ëŸ‰: %dê°œ (ê³ ìœ )", market, len(uni_df))
        selected = _calc_best_k_for_universe(uni_df, rebalance_date, forced_codes, market=market)
        logger.info("[SELECT] %s ìµœì¢… ì„ ì • %dê°œ", market, len(selected))
        all_selected.extend(selected)

    if not all_selected:
        return {"selected": [], "selected_by_market": {}} if return_by_market else []

    by_market = _normalize_weights_by_market(all_selected, forced_codes)
    merged_per_market: List[Dict[str, Any]] = []
    for rows in by_market.values():
        merged_per_market.extend(rows)

    merged_global: List[Dict[str, Any]] = [dict(r) for r in merged_per_market]
    total_weight = sum(float(it.get("weight") or 0.0) for it in merged_global)
    if total_weight > 0:
        for it in merged_global:
            it["weight"] = float(it.get("weight") or 0.0) / total_weight

    counts: Dict[str, int] = {}
    for row in merged_per_market:
        mkt = row.get("market") or "UNKNOWN"
        counts[mkt] = counts.get(mkt, 0) + 1
    for mkt, cnt in counts.items():
        logger.info("[COUNT] %s selected_count=%d", mkt, cnt)

    if return_by_market:
        return {
            "selected": merged_global,
            "selected_by_market": by_market,
            "weight_scope": {"selected": "global", "selected_by_market": "per_market"},
        }
    return merged_global


# Backward compatibility alias for callers that still want the KOSDAQ-only variant
get_best_k_for_kosdaq_only = get_best_k_for_kosdaq_topn

# -----------------------------
# 5) API ì§„ì…ì : /rebalance/run/{date} ì—ì„œ í˜¸ì¶œ
# -----------------------------
def run_rebalance(
    date: str, force_order: bool = False, return_by_market: bool = False
) -> Dict[str, Any]:
    """
    /rebalance/run/{date} ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì§ì ‘ í˜¸ì¶œë˜ëŠ” ì§„ì…ì .
    ë°˜í™˜ ìŠ¤í‚¤ë§ˆëŠ” trader.py/main.pyê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ë³´ì¥í•œë‹¤.

    Returns:
        {
          "selected": [ ... ],
          "selected_stocks": [ ... ]  # ë™ì¼ ë°°ì—´(í˜¸í™˜ì„±)
        }
    """
    try:
        results = get_best_k_for_krx_topn(date, return_by_market=return_by_market)
        if isinstance(results, dict):
            selected = results.get("selected", [])
            selected_by_market = results.get("selected_by_market", {})
            weight_scope = results.get("weight_scope")
        else:
            selected = results
            selected_by_market = {}
            weight_scope = None
        # force_orderê°€ Trueë¼ê³  í•´ì„œ ì—¬ê¸°ì„œ ì‹¤ì£¼ë¬¸ì„ ë‚´ì§€ ì•ŠìŒ.
        # (ì£¼ë¬¸ì€ trader.pyê°€ ê´€ë¦¬) â€” í•„ìš” ì‹œ 'strategy'ì— í”Œë˜ê·¸ë§Œ ë‚¨ê¹€.
        for it in selected:
            it.setdefault("strategy", "ì „ì›” rolling K ìµœì í™”")
    except Exception as e:
        logger.exception("[run_rebalance] failed: %s", e)
        selected = []
        selected_by_market = {}
        weight_scope = None

    payload: Dict[str, Any] = {
        "selected": selected,
        "selected_stocks": selected,
        "selected_by_market": selected_by_market,
    }
    if weight_scope:
        payload["weight_scope"] = weight_scope
    return payload
