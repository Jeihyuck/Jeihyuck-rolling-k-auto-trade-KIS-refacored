diff --git a/.github/workflows/trade-runner.yml b/.github/workflows/trade-runner.yml
index cae7b384ed586daa7314e11e7525997b37a39f05..49ce41622236cf0c9fd6a644e2289c2bfc9cc4a9 100644
--- a/.github/workflows/trade-runner.yml
+++ b/.github/workflows/trade-runner.yml
@@ -26,50 +26,51 @@ on:
     # =========================
     # Weekend (KST Sat-Sun) : DIAG every 5 min for the entire weekend day (00:00~23:55 KST)
     # KST Sat 00:00~08:55 => UTC Fri 15:00~23:55
     - cron: "*/5 15-23 * * 5"
     # KST Sat 09:00~Sun 08:55 => UTC Sat 00:00~23:55 (covers full UTC Saturday)
     - cron: "*/5 * * * 6"
     # KST Sun 09:00~23:55 => UTC Sun 00:00~14:55
     - cron: "*/5 0-14 * * 0"
 
 jobs:
   run-trade:
     if: github.ref != 'refs/heads/bot-state' && github.event_name != 'pull_request'
     runs-on: ubuntu-latest
     # Note: cancel-in-progress=false relies on lock file; timeout increased to allow pre-open waits.
     timeout-minutes: 360
     env:
       BOTSTATE_BRANCH: bot-state
       BOTSTATE_WORKTREE_DIR: "_botstate"
       STRATEGY_MODE: "LIVE"
       LIVE_TRADING_ENABLED: "1"
       DISABLE_LIVE_TRADING: "0"
       DRY_RUN: "0"
       EXPECT_LIVE_TRADING: "1"
       EXPECT_KIS_ENV: "practice"
       PB1_ENTRY_ENABLED: "1"
+      PB1_SKIP_UNIVERSE_IN_DIAG: "1"
       KIS_ENV: "practice"
       API_BASE_URL: "https://openapivts.koreainvestment.com:29443"
       KIS_APP_KEY: ${{ secrets.KIS_APP_KEY != '' && secrets.KIS_APP_KEY || secrets.APP_KEY }}
       KIS_APP_SECRET: ${{ secrets.KIS_APP_SECRET != '' && secrets.KIS_APP_SECRET || secrets.APP_SECRET }}
       APP_KEY: ${{ secrets.KIS_APP_KEY != '' && secrets.KIS_APP_KEY || secrets.APP_KEY }}
       APP_SECRET: ${{ secrets.KIS_APP_SECRET != '' && secrets.KIS_APP_SECRET || secrets.APP_SECRET }}
       CANO: ${{ secrets.CANO }}
       ACNT_PRDT_CD: ${{ secrets.ACNT_PRDT_CD }}
       STATE_RECONCILE_APPLY: "1"
       MORNING_WINDOW_START: "08:50"
       MORNING_WINDOW_END: "11:00"
       MORNING_EXIT_START: "09:00"
       MORNING_EXIT_END: "09:20"
       AFTERNOON_WINDOW_START: "14:00"
       AFTERNOON_WINDOW_END: "15:30"
       CLOSE_AUCTION_START: "15:20"
       CLOSE_AUCTION_END: "15:30"
       MARKET_OPEN_HHMM: "08:50"
       MARKET_CLOSE_HHMM: "15:30"
       PB1_WAIT_FOR_WINDOW: "1"
       PB1_MAX_WAIT_FOR_WINDOW_MIN: "240"
       LEDGER_BASE_DIR: "bot_state/trader_ledger"
       LEDGER_LOOKBACK_DAYS: "120"
       BOTSTATE_LOCK_TTL_SEC: "1800"
       PYTHONUNBUFFERED: "1"
diff --git a/rolling_k_auto_trade_api/best_k_meta_strategy.py b/rolling_k_auto_trade_api/best_k_meta_strategy.py
index 10329877ec5606785381945b536c83397f083ea2..85494af7428578f934bfa0f26391ffbc2188f9d1 100644
--- a/rolling_k_auto_trade_api/best_k_meta_strategy.py
+++ b/rolling_k_auto_trade_api/best_k_meta_strategy.py
@@ -1,179 +1,257 @@
 # -*- coding: utf-8 -*-
 # best_k_meta_strategy.py (Ïã§Ï†Ñ rolling_k, ÏµúÏ†ÅÌôî Ï†ÑÏ≤¥Î≥∏)
 """
 Ïã§Ï†ÑÌòï rolling_k Î≥ÄÎèôÏÑ±ÎèåÌåå + ÏõîÏ¥à/rolling/TopN/Î≥¥Ïú†Î∂Ñ/ÎèôÏ†ÅK/Í∞ÄÏ§ëÏπò ÏµúÏ†ÅÌôî Ï†ÑÎûµ
 - KOSDAQ TopN(pykrx+fdr) Ïú†ÎãàÎ≤ÑÏä§/ÏãúÏ¥ù ÎèôÏ†Å
 - Ïõî/Î∂ÑÍ∏∞/Ïó∞Í∞Ñ K-grid(Í≥†Ï†ï/ATRÎèôÏ†Å)
 - Î™©ÌëúÍ∞Ä: Ï†ÑÏùº Î≥ÄÎèôÌè≠*K + Ìã±Î≥¥Ï†ï
 - best_k/Sharpe/ÏäπÎ•†/ÏàòÏùµÎ•†/MDD/Í±∞ÎûòÏàò ÌïÑÌÑ∞ + assign_weights
 - Î≥¥Ïú†Ï¢ÖÎ™© Í∞ïÏ†úÌè¨Ìï®/ÎπÑÏ§ëÌïòÌïú/rolling ÌÜµÌï©
 - FastAPI(trader.py/main.py)ÏóêÏÑú /rebalance/run/{date}Í∞Ä Ìò∏Ï∂úÌï† run_rebalance() Ï†úÍ≥µ
 """
 
 from __future__ import annotations
 
 import logging
 import math
 import os
 from datetime import datetime, timedelta, date
 from functools import lru_cache
+from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional
+import json
 
 import numpy as np
 import pandas as pd
 import FinanceDataReader as fdr
+import requests
 from pykrx.stock import (
     get_market_cap_by_ticker,
     get_nearest_business_day_in_a_week,
 )
+try:  # pykrx wrapperÏùò ÏûòÎ™ªÎêú logging Ìè¨Îß∑ÏúºÎ°ú Ïù∏Ìïú Î°úÍ∑∏ Ìè≠Ï£º Î∞©ÏßÄ
+    from pykrx.website import comm as _pykrx_comm  # type: ignore
+
+    if hasattr(_pykrx_comm, "logging") and hasattr(_pykrx_comm.logging, "info"):
+        _pykrx_comm.logging.info = lambda *_, **__: None  # type: ignore
+except Exception:
+    # pykrxÍ∞Ä ÏóÜÍ±∞ÎÇò ÎÇ¥Î∂Ä Íµ¨Ï°∞ Î≥ÄÍ≤Ω ÏãúÏóêÎèÑ Îü∞Ïù¥ Í≥ÑÏÜçÎêòÎèÑÎ°ù Î¨¥Ïãú
+    pass
 
 from trader.rkmax_utils import get_best_k_meta, assign_weights, _enforce_min_weight_for_forced
 from .simulate_with_k_and_get_metrics import simulate_with_k_and_get_metrics
 from rolling_k_auto_trade_api.adjust_price_to_tick import adjust_price_to_tick
 
 logger = logging.getLogger(__name__)
 
 # -----------------------------
 # ÌôòÍ≤Ω ÌååÎùºÎØ∏ÌÑ∞ (ÌäúÎãù Í∞ÄÎä•)
 # -----------------------------
 K_MIN = float(os.getenv("K_MIN", "0.1"))
 K_MAX = float(os.getenv("K_MAX", "1.0"))
 K_STEP = float(os.getenv("K_STEP", "0.1"))
 K_GRID_MODE = os.getenv("K_GRID_MODE", "fixed").lower()  # fixed|fine|atr
 K_STEP_FINE = float(os.getenv("K_STEP_FINE", "0.05"))
 K_DYNAMIC_STEP_MIN = float(os.getenv("K_DYNAMIC_STEP_MIN", "0.03"))
 K_DYNAMIC_STEP_MAX = float(os.getenv("K_DYNAMIC_STEP_MAX", "0.10"))
 K_DYNAMIC_STEP_MULT = float(os.getenv("K_DYNAMIC_STEP_MULT", "1.5"))
 
 MIN_TRADES = int(os.getenv("MIN_TRADES", "5"))
 MAX_MDD_PCT = float(os.getenv("MAX_MDD_PCT", "30"))
 REQUIRE_POS_RET = os.getenv("REQUIRE_POS_RET", "true").lower() == "true"
 
 TOP_N = int(os.getenv("TOP_N", "50"))
 
 ALWAYS_INCLUDE_CODES = {
     c.strip() for c in os.getenv("ALWAYS_INCLUDE_CODES", "").replace(" ", "").split(",") if c.strip()
 }
 KEEP_HELD_BYPASS_FILTERS = os.getenv("KEEP_HELD_BYPASS_FILTERS", "true").lower() == "true"
 HELD_MIN_WEIGHT = float(os.getenv("HELD_MIN_WEIGHT", "0.01"))
+UNIVERSE_CACHE_ENV = "UNIVERSE_CACHE_DIR"
+UNIVERSE_CACHE_SUBDIR = "universe_cache"
 
 # -----------------------------
 # Ïú†Ìã∏
 # -----------------------------
 def _clip(v: float, lo: float, hi: float) -> float:
     return max(lo, min(hi, v))
 
 def _round2(x: float) -> float:
     return float(np.round(x, 2))
 
 def _safe_float(x: Any, default: float | None = 0.0) -> float | None:
     try:
         return float(x)
     except Exception:
         return default
 
 def _find_column(df: pd.DataFrame, keyword: str) -> Optional[str]:
     kw = keyword.replace(" ", "")
     for c in df.columns:
         if kw in str(c).replace(" ", ""):
             return c
     return None
 
 # -----------------------------
 # 1) ÏãúÍ∞ÄÏ¥ùÏï° Í∏∞Ï§Ä Top-N (KOSDAQ only for rolling-k universe)
 # -----------------------------
 @lru_cache(maxsize=None)
 def _get_listing_df_cached(markets: tuple[str, ...]) -> pd.DataFrame:
     frames: List[pd.DataFrame] = []
     for m in markets:
         try:
             df = fdr.StockListing(m).rename(columns={"Symbol": "Code", "Name": "Name"})
             df["Code"] = df["Code"].astype(str).str.zfill(6)
             frames.append(df[["Code", "Name"]])
         except Exception:
             logger.exception("‚ùå  StockListing(%s) Ïã§Ìå®", m)
     if not frames:
         return pd.DataFrame(columns=["Code", "Name"])
     merged = pd.concat(frames, ignore_index=True)
     merged = merged.drop_duplicates(subset=["Code"], keep="first")
     return merged
 
 
 def _get_listing_df(markets: Iterable[str]) -> pd.DataFrame:
     """Ï£ºÏñ¥ÏßÑ ÏãúÏû• Î¶¨Ïä§Ìä∏Ïóê ÎåÄÌïú Ï¢ÖÎ™©Î™Ö DF Ìï©Ïπú ÌõÑ Code Ìè¨Îß∑ÏùÑ Ï†ïÍ∑úÌôîÌïúÎã§."""
     normalized_markets = tuple(dict.fromkeys(markets))
     return _get_listing_df_cached(normalized_markets).copy()
 
+
+def _universe_cache_base() -> Path:
+    explicit = os.getenv(UNIVERSE_CACHE_ENV)
+    if explicit:
+        return Path(explicit)
+    base_dir = Path(os.getenv("LEDGER_BASE_DIR", "bot_state/trader_ledger"))
+    if not base_dir.is_absolute():
+        base_dir = Path.cwd() / base_dir
+    return base_dir / UNIVERSE_CACHE_SUBDIR
+
+
+def _universe_cache_path(market: str) -> Path:
+    return _universe_cache_base() / market / "latest.json"
+
+
+def _load_cached_universe(market: str) -> pd.DataFrame:
+    path = _universe_cache_path(market)
+    try:
+        if path.exists():
+            payload = json.loads(path.read_text())
+            df = pd.DataFrame(payload)
+            if not df.empty:
+                df["Code"] = df["Code"].astype(str).str.zfill(6)
+            return df
+    except Exception:
+        logger.warning("‚ö†Ô∏è  %s Ï∫êÏãú Î°úÎìú Ïã§Ìå® ‚Üí Îπà DF ÏÇ¨Ïö©", market, exc_info=logger.isEnabledFor(logging.DEBUG))
+    return pd.DataFrame(columns=["Code", "Name", "Marcap"])
+
+
+def _save_cached_universe(df: pd.DataFrame, market: str) -> None:
+    if df is None or df.empty:
+        return
+    path = _universe_cache_path(market)
+    path.parent.mkdir(parents=True, exist_ok=True)
+    tmp_path = path.with_suffix(".tmp")
+    try:
+        tmp_path.write_text(df.to_json(orient="records", force_ascii=False))
+        tmp_path.replace(path)
+        logger.info("üíæ %s Ïú†ÎãàÎ≤ÑÏä§ Ï∫êÏãú Ï†ÄÏû• %s", market, path)
+    except Exception:
+        logger.warning("‚ö†Ô∏è  %s Ï∫êÏãú Ï†ÄÏû• Ïã§Ìå®", market, exc_info=logger.isEnabledFor(logging.DEBUG))
+        try:
+            if tmp_path.exists():
+                tmp_path.unlink()
+        except Exception:
+            pass
+
 def _get_top_n_for_market(date_str: Optional[str], n: int, market: str) -> pd.DataFrame:
     """Ï£ºÏñ¥ÏßÑ ÏãúÏû•Ïùò ÏãúÍ∞ÄÏ¥ùÏï° ÏÉÅÏúÑ nÍ∞ú Ï¢ÖÎ™© Î∞òÌôò."""
+    cached = _load_cached_universe(market)
     try:
         target_dt = datetime.today() if date_str is None else datetime.strptime(date_str, "%Y-%m-%d")
         from_date = get_nearest_business_day_in_a_week(target_dt.strftime("%Y%m%d"))
         logger.info(f"üìÖ pykrx ÏãúÏ¥ù Ï°∞ÌöåÏùº({market}) ‚Üí {from_date}")
 
         mktcap_df = get_market_cap_by_ticker(from_date, market=market)
         if mktcap_df is None or len(mktcap_df) == 0:
             logger.warning("‚ö†Ô∏è  pykrx ÏãúÏ¥ù DF(%s)Í∞Ä ÎπÑÏóàÏäµÎãàÎã§ ‚Üí Îπà DF Î∞òÌôò", market)
-            return pd.DataFrame(columns=["Code", "Name", "Marcap"])
+            return cached if not cached.empty else pd.DataFrame(columns=["Code", "Name", "Marcap"])
 
         mktcap_df = mktcap_df.reset_index()
         capcol = _find_column(mktcap_df, "ÏãúÍ∞ÄÏ¥ùÏï°")
         ticcol = _find_column(mktcap_df, "Ìã∞Ïª§") or _find_column(mktcap_df, "ÏΩîÎìú")
         if capcol is None or ticcol is None:
             logger.error("‚ùå  %s ÏãúÏ¥ù/Ìã∞Ïª§ Ïª¨Îüº ÌÉêÏÉâ Ïã§Ìå® ‚Üí Îπà DF Î∞òÌôò", market)
-            return pd.DataFrame(columns=["Code", "Name", "Marcap"])
+            return cached if not cached.empty else pd.DataFrame(columns=["Code", "Name", "Marcap"])
 
         mktcap_df = mktcap_df.rename(columns={capcol: "Marcap", ticcol: "Code"})
         mktcap_df["Code"] = mktcap_df["Code"].astype(str).str.zfill(6)
 
         fdr_df = _get_listing_df([market])
         merged = pd.merge(
             fdr_df[["Code", "Name"]],
             mktcap_df[["Code", "Marcap"]],
             on="Code",
             how="inner",
         )
         if "Marcap" not in merged.columns:
             for cand in ("Marcap_x", "Marcap_y", "MarketCap", "MarketCap_x", "MarketCap_y"):
                 if cand in merged.columns:
                     merged = merged.rename(columns={cand: "Marcap"})
                     break
         if "Marcap" not in merged.columns:
             logger.error("‚ùå  Î≥ëÌï© ÌõÑÏóêÎèÑ 'Marcap' ÏóÜÏùå(%s) ‚Üí Îπà DF Î∞òÌôò", market)
-            return pd.DataFrame(columns=["Code", "Name", "Marcap"])
+            return cached if not cached.empty else pd.DataFrame(columns=["Code", "Name", "Marcap"])
 
         topn = merged.dropna(subset=["Marcap"])
         # 6ÏûêÎ¶¨ Ïà´Ïûê ÏΩîÎìúÎßå ÏÇ¨Ïö© (Ïö∞ÏÑ†Ï£º/ETN Îì± ÌäπÏàòÏΩîÎìú, 0009K0 Í∞ôÏùÄ Í≤É Ï†úÍ±∞)
         topn = topn[topn["Code"].astype(str).str.match(r"^\d{6}$")]
         topn = topn.sort_values("Marcap", ascending=False).head(n)
         logger.info(f"‚úÖ  {market} ÏãúÏ¥ù Top{n} Ï∂îÏ∂ú ÏôÑÎ£å ‚Üí {len(topn)} Ï¢ÖÎ™©")
-        return topn[["Code", "Name", "Marcap"]]
-
+        result = topn[["Code", "Name", "Marcap"]]
+        if result.empty and not cached.empty:
+            logger.warning("‚ö†Ô∏è  %s TopN Í≤∞Í≥ºÍ∞Ä ÎπÑÏñ¥ Ï∫êÏãú ÏÇ¨Ïö©(%d rows)", market, len(cached))
+            return cached
+        _save_cached_universe(result, market)
+        return result
+
+    except (
+        requests.exceptions.JSONDecodeError,
+        json.decoder.JSONDecodeError,
+        IndexError,
+        ValueError,
+    ) as exc:
+        logger.warning("‚ö†Ô∏è  %s pykrx Ï°∞Ìöå Ïã§Ìå® ‚Üí Ï∫êÏãú Ìè¥Î∞± ÏãúÎèÑ: %s", market, exc)
     except Exception:
-        logger.exception("‚ùå  get_top_n_for_market(%s) ÏòàÏô∏:", market)
-        return pd.DataFrame(columns=["Code", "Name", "Marcap"])
+        logger.warning("‚ö†Ô∏è  get_top_n_for_market(%s) ÏòàÏô∏ Î∞úÏÉù ‚Üí Ï∫êÏãú Ìè¥Î∞±", market, exc_info=logger.isEnabledFor(logging.DEBUG))
+
+    if not cached.empty:
+        logger.info("‚Ü©Ô∏è  %s Ïú†ÎãàÎ≤ÑÏä§ Ï∫êÏãú ÏÇ¨Ïö© (%d rows)", market, len(cached))
+        return cached
+    logger.warning("‚ö†Ô∏è  %s Ï∫êÏãú ÏóÜÏùå ‚Üí Îπà DF Î∞òÌôò", market)
+    return pd.DataFrame(columns=["Code", "Name", "Marcap"])
 
 def get_kosdaq_top_n(date_str: Optional[str] = None, n: int = TOP_N) -> pd.DataFrame:
     return _get_top_n_for_market(date_str, n, market="KOSDAQ")
 
 def get_kospi_top_n(date_str: Optional[str] = None, n: int = TOP_N) -> pd.DataFrame:
     return _get_top_n_for_market(date_str, n, market="KOSPI")
 
 # -----------------------------
 # ATR Í≥ÑÏÇ∞(Ïõî Îç∞Ïù¥ÌÑ∞ Î†àÏΩîÎìúÏóêÏÑú)
 # -----------------------------
 def _compute_atr_from_records(records: List[Dict[str, Any]], window: int = 14) -> Optional[float]:
     """Ïõî Íµ¨Í∞Ñ Î†àÏΩîÎìú([{open,high,low,close}...])ÏóêÏÑú ATR Í≥ÑÏÇ∞."""
     if not records or len(records) < window + 1:
         return None
     df = pd.DataFrame(records).copy()
     need = {"open", "high", "low", "close"}
     if not need.issubset(set(df.columns)):
         return None
     df = df[["open", "high", "low", "close"]].astype(float)
     prev_close = df["close"].shift(1)
     tr = pd.concat(
         [
             (df["high"] - df["low"]).abs(),
             (df["high"] - prev_close).abs(),
             (df["low"] - prev_close).abs(),
diff --git a/tests/test_diag_skips_universe.py b/tests/test_diag_skips_universe.py
new file mode 100644
index 0000000000000000000000000000000000000000..e0d6d79781c0c9b81867caea1f7a867f7c5181cc
--- /dev/null
+++ b/tests/test_diag_skips_universe.py
@@ -0,0 +1,42 @@
+import os
+from pathlib import Path
+
+import pytest
+
+from trader.pb1_engine import PB1Engine
+from trader.window_router import WindowDecision
+
+
+@pytest.fixture(autouse=True)
+def _ensure_env(monkeypatch, tmp_path):
+    monkeypatch.setenv("PB1_SKIP_UNIVERSE_IN_DIAG", "1")
+    # Avoid git operations during tests
+    monkeypatch.setattr("trader.pb1_engine.persist_run_files", lambda *args, **kwargs: None)
+    # Keep ledger under a temp directory
+    monkeypatch.setattr("trader.pb1_engine.LEDGER_BASE_DIR", Path(tmp_path / "ledger"))
+
+
+def test_diagnostic_window_skips_universe(monkeypatch, tmp_path):
+    window = WindowDecision(name="diagnostic", phase="verify")
+    engine = PB1Engine(
+        kis=None,
+        worktree_dir=tmp_path,
+        window=window,
+        phase_override="auto",
+        dry_run=True,
+        env="paper",
+        run_id="test-run",
+    )
+
+    called = {"build": False}
+
+    def _fail_build():
+        called["build"] = True
+        raise AssertionError("_build_universe should not run in diagnostic")
+
+    monkeypatch.setattr(engine, "_build_universe", _fail_build)
+
+    touched = engine.run()
+
+    assert called["build"] is False
+    assert any(p.name == "pnl_snapshot.json" and p.exists() for p in touched)
diff --git a/tests/test_pykrx_fallback.py b/tests/test_pykrx_fallback.py
new file mode 100644
index 0000000000000000000000000000000000000000..7e2af988684e930f3bc1a2545a2cc19f95d51844
--- /dev/null
+++ b/tests/test_pykrx_fallback.py
@@ -0,0 +1,48 @@
+import json
+from pathlib import Path
+
+import pandas as pd
+import pytest
+import requests
+
+from rolling_k_auto_trade_api import best_k_meta_strategy as strat
+
+
+@pytest.fixture
+def cache_dir(tmp_path, monkeypatch):
+    path = tmp_path / "universe_cache"
+    monkeypatch.setenv("UNIVERSE_CACHE_DIR", str(path))
+    strat._get_listing_df_cached.cache_clear()
+    return path
+
+
+def test_pykrx_failure_uses_cached_universe(cache_dir, monkeypatch):
+    cached_df = pd.DataFrame([{"Code": "123456", "Name": "Alpha", "Marcap": 1000}])
+    strat._save_cached_universe(cached_df, "KOSPI")
+
+    monkeypatch.setattr(strat, "_get_listing_df", lambda markets: cached_df[["Code", "Name"]])
+    monkeypatch.setattr(strat, "get_nearest_business_day_in_a_week", lambda *_args, **_kwargs: "20240102")
+
+    def raise_decode(*_args, **_kwargs):
+        raise json.JSONDecodeError("bad json", doc="", pos=0)
+
+    monkeypatch.setattr(strat, "get_market_cap_by_ticker", raise_decode)
+
+    result = strat._get_top_n_for_market("2024-01-02", n=1, market="KOSPI")
+
+    assert not result.empty
+    assert result.iloc[0]["Code"] == "123456"
+
+
+def test_pykrx_failure_without_cache_returns_empty(cache_dir, monkeypatch):
+    monkeypatch.setattr(strat, "_get_listing_df", lambda markets: pd.DataFrame(columns=["Code", "Name"]))
+    monkeypatch.setattr(strat, "get_nearest_business_day_in_a_week", lambda *_args, **_kwargs: "20240102")
+
+    def raise_network(*_args, **_kwargs):
+        raise requests.exceptions.JSONDecodeError("bad json", response=None)
+
+    monkeypatch.setattr(strat, "get_market_cap_by_ticker", raise_network)
+
+    result = strat._get_top_n_for_market("2024-01-02", n=5, market="KOSDAQ")
+
+    assert result.empty
diff --git a/trader/pb1_engine.py b/trader/pb1_engine.py
index 43c67c16534fb2a44f1cd7ef46b447ec0b7aa518..746212d19d57d45079f9121d3625bfa7e4c7e2e8 100644
--- a/trader/pb1_engine.py
+++ b/trader/pb1_engine.py
@@ -1,29 +1,30 @@
 from __future__ import annotations
 
 import logging
 from dataclasses import dataclass
+import os
 from datetime import datetime, timedelta
 from pathlib import Path
 from typing import Dict, Iterable, List, Tuple
 
 import pandas as pd
 
 from rolling_k_auto_trade_api.best_k_meta_strategy import run_rebalance
 from trader.config import (
     CAP_CAP,
     DAILY_CAPITAL,
     KOSDAQ_HARD_STOP_PCT,
     KOSPI_HARD_STOP_PCT,
     LEDGER_BASE_DIR,
     LEDGER_LOOKBACK_DAYS,
     PB1_ENTRY_ENABLED,
     PB1_DAY_SL_R,
     PB1_DAY_TP_R,
     PB1_R_FLOOR_PCT,
     PB1_TIME_STOP_DAYS,
     PB1_REQUIRE_VOLUME,
 )
 from trader.utils.env import env_bool
 from trader.kis_wrapper import KisAPI
 from trader.ledger.event_types import new_error, new_exit_intent, new_order_intent, new_fill, new_order_ack, new_unfilled
 from trader.ledger.store import LedgerStore
@@ -459,60 +460,89 @@ class PB1Engine:
                 client_order_key=client_key,
                 reasons=ack.reasons if ack.reasons else ["order_failed"],
                 stage=stage,
             )
             self.ledger.append_event("errors", err)
 
     def _positions_with_meta(self, positions: Dict[Tuple[str, int, int], Dict]) -> List[Dict]:
         enriched: List[Dict] = []
         for (code, sid, mode), state in positions.items():
             if sid != 1:
                 continue
             enriched.append(
                 {
                     "code": code,
                     "sid": sid,
                     "mode": mode,
                     "total_qty": state.get("total_qty") or 0,
                     "avg_buy_price": state.get("avg_buy_price"),
                     "market": state.get("market"),
                     "holding_days": state.get("holding_days") or 0,
                     "first_buy_ts": state.get("first_buy_ts"),
                 }
             )
         return enriched
 
+    def _is_diagnostic_guard_enabled(self) -> bool:
+        skip_env = os.getenv("PB1_SKIP_UNIVERSE_IN_DIAG") == "1"
+        return bool(
+            (self.window and self.window.name == "diagnostic")
+            or (self.dry_run and skip_env)
+            or (self.phase == "verify")
+        )
+
+    def _run_verify_only(self, positions: Dict[Tuple[str, int, int], Dict], touched: List[Path]) -> List[Path]:
+        logger.info(
+            "[PB1][DIAG-MODE] universe/rebalance skipped window=%s phase=%s dry_run=%s",
+            self.window.name,
+            self.phase,
+            self.dry_run,
+        )
+        marks = self._fetch_marks([p["code"] for p in self._positions_with_meta(positions)], {})
+        snapshot = self.ledger.generate_pnl_snapshot(positions, marks=marks)
+        logger.info(
+            "[PNL][SNAPSHOT] portfolio_return_pct=%.2f%% unrealized=%.2f realized=%.2f",
+            snapshot["totals"]["portfolio_return_pct"],
+            snapshot["totals"]["unrealized"],
+            snapshot["totals"]["realized"],
+        )
+        snap_path = self.ledger.write_snapshot(snapshot, self.run_id)
+        touched.append(snap_path)
+        return touched
+
     def run(self) -> List[Path]:
         entry_allowed = PB1_ENTRY_ENABLED and env_bool("PB1_ENTRY_ENABLED", PB1_ENTRY_ENABLED)
         if not entry_allowed:
             logger.warning("[PB1][ENTRY_DISABLED] PB1_ENTRY_ENABLED=%s -> skip new entries", entry_allowed)
         logger.info("[PB1][RUN] window=%s phase=%s dry_run=%s", self.window.name, self.phase, self.dry_run)
         run_files = self.ledger.open_run_files()
         touched: List[Path] = list(run_files.values())
         logger.info("[LEDGER][APPEND] kind=touch path=%s", run_files)
         persist_run_files(self.worktree_dir, touched, message=f"pb1 touch run_id={self.run_id}")
         positions = self.ledger.rebuild_positions_average_cost(lookback_days=LEDGER_LOOKBACK_DAYS)
+        if self._is_diagnostic_guard_enabled():
+            return self._run_verify_only(positions, touched)
         selected = self._build_universe()
         code_market = self._code_market_map(selected)
         marks_fallback: Dict[str, float] = {}
         if self.phase in {"prep", "entry"}:
             candidates = self._compute_candidates(selected)
             candidates = self._size_positions(candidates)
             if self.phase == "entry" and self.window.name == "afternoon":
                 code_hint = candidates[0].code if candidates else next(iter(code_market), "005930")
                 price_hint = candidates[0].features.get("close") if candidates else None
                 orderable_cash = 0
                 cash_meta: Dict[str, object] = {"source": "none", "raw_fields": {}, "clamp_applied": False}
                 if self.kis:
                     orderable_cash, cash_meta = self.kis.get_orderable_cash(code_hint, price_hint)
                 if orderable_cash < 0:
                     orderable_cash = 0
                 logger.info(
                     "[PB1][CASH][ORDERABLE] value=%s source=%s clamp=%s raw_fields=%s",
                     orderable_cash,
                     cash_meta.get("source"),
                     cash_meta.get("clamp_applied"),
                     {
                         k: (cash_meta.get("raw_fields") or {}).get(k)
                         for k in ("ord_psbl_cash", "ord_psbl_amt", "nrcvb_buy_amt", "dnca_tot_amt")
                     },
                 )
